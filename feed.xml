<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tursunboev.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tursunboev.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-02T05:05:36+00:00</updated><id>https://tursunboev.com/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Jamshid Tursunboev
</subtitle><entry><title type="html">Implementing Pytorch-like arbitrary dimensional Tensor in Go</title><link href="https://tursunboev.com/blog/2024/tensor-in-golang/" rel="alternate" type="text/html" title="Implementing Pytorch-like arbitrary dimensional Tensor in Go" /><published>2024-02-28T00:00:00+00:00</published><updated>2024-02-28T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2024/tensor-in-golang</id><content type="html" xml:base="https://tursunboev.com/blog/2024/tensor-in-golang/"><![CDATA[<p>In one of the <a href="https://karpathy.ai/zero-to-hero.html">Karpathy’s videos</a>, he recommends the blogpost by Edward Z. Yang called <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals</a> to learn about the implementation of Tensor class in Pytorch. The blogpost is fantastic and goes into detailed representation of some of the basic functionality of Tensors.</p>

<p>After reading the post, I tried to implement similar functionality in <a href="https://go.dev/">Go</a> using slices. In this post, I will walk through following methods of basic <code class="language-plaintext highlighter-rouge">Tensor</code> stuct:</p>
<ul>
  <li><a href="#creating-newtensor-from-data-or-empty">NewTensor (Creating empty Tensor)</a></li>
  <li><a href="#accessing-the-single-element-with-at">At (indexing using arbitrary number of dimensions)</a></li>
  <li><a href="#viewing-the-tensor-with-different-sizes">View (Returning different size tensor with the same underlying slice)</a></li>
  <li><a href="#slicing-the-tensor-along-certain-dimension">DimSlice (hardest: Getting the slice of arbitrary dimension <code class="language-plaintext highlighter-rouge">a[:, :, 2, :]</code>)</a></li>
</ul>

<p><em>Full implementation code and tests can be found <a href="https://github.com/gladuz/deep-learning-go/tree/main/tensor">this repository</a>.</em></p>
<h1 id="tensor-struct">Tensor struct</h1>
<p>Basic Tensor in Pytorch has the <code class="language-plaintext highlighter-rouge">data</code> in one dimensional continous array, and <code class="language-plaintext highlighter-rouge">sizes, strides, offset</code> for getting the multi-dimensional view of the Tensor.
For detailed explanation, please refer to <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals</a>.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/slide-08-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/slide-08-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/slide-08-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/slide-08.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Tensor representation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">http://blog.ezyang.com/2019/05/pytorch-internals/</div>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">type</span> <span class="n">Tensor</span> <span class="k">struct</span> <span class="p">{</span>
	<span class="n">Sizes</span>   <span class="p">[]</span><span class="kt">int</span>
	<span class="n">Strides</span> <span class="p">[]</span><span class="kt">int</span>
	<span class="n">Offset</span>  <span class="kt">int</span>
	<span class="n">Data</span>    <span class="p">[]</span><span class="kt">float64</span>
<span class="p">}</span>
</code></pre></div></div>
<h2 id="creating-newtensor-from-data-or-empty">Creating NewTensor from data or empty</h2>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="n">NewTensor</span><span class="p">(</span><span class="n">data</span> <span class="p">[]</span><span class="kt">float64</span><span class="p">,</span> <span class="n">sizes</span> <span class="p">[]</span><span class="kt">int</span><span class="p">)</span> <span class="o">*</span><span class="n">Tensor</span> <span class="p">{</span>
	<span class="n">t</span> <span class="o">:=</span> <span class="n">Tensor</span><span class="p">{}</span>
	<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">==</span> <span class="m">0</span> <span class="p">{</span>
		<span class="n">t</span><span class="o">.</span><span class="n">Data</span> <span class="o">=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="m">0</span><span class="p">)</span>
		<span class="n">t</span><span class="o">.</span><span class="n">Sizes</span> <span class="o">=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span> <span class="m">0</span><span class="p">)</span>
		<span class="n">t</span><span class="o">.</span><span class="n">Offset</span> <span class="o">=</span> <span class="m">0</span>
		<span class="n">t</span><span class="o">.</span><span class="n">Strides</span> <span class="o">=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span> <span class="m">0</span><span class="p">)</span>
		<span class="k">return</span> <span class="o">&amp;</span><span class="n">t</span>
	<span class="p">}</span>
	<span class="n">t</span><span class="o">.</span><span class="n">Data</span> <span class="o">=</span> <span class="n">data</span>
	<span class="n">t</span><span class="o">.</span><span class="n">Sizes</span> <span class="o">=</span> <span class="n">sizes</span>
	<span class="n">t</span><span class="o">.</span><span class="n">Offset</span> <span class="o">=</span> <span class="m">0</span>

	<span class="n">t</span><span class="o">.</span><span class="n">Strides</span> <span class="o">=</span> <span class="n">calculateStrides</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="n">t</span>
<span class="p">}</span>
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Sizes</code> is the dimension sizes (same as <code class="language-plaintext highlighter-rouge">.dim</code> or <code class="language-plaintext highlighter-rouge">.size()</code> in Pytorch)</li>
  <li><code class="language-plaintext highlighter-rouge">Strides</code> is the required stride of each dimension, meaning how many elements should we skip to get to next element. 
For example: how many elements between <code class="language-plaintext highlighter-rouge">a[1, 0]</code> and <code class="language-plaintext highlighter-rouge">a[2, 0]</code>.</li>
</ul>

<p>To calculate the strides of Tensor given its sizes, we reverse multiply each dimension until the first one.</p>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="n">calculateStrides</span><span class="p">(</span><span class="n">sizes</span> <span class="p">[]</span><span class="kt">int</span><span class="p">)</span> <span class="p">[]</span><span class="kt">int</span> <span class="p">{</span>
	<span class="n">strides</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">))</span>
	<span class="n">inter_stride</span> <span class="o">:=</span> <span class="m">1</span>
	<span class="k">for</span> <span class="n">i</span> <span class="o">:=</span> <span class="nb">len</span><span class="p">(</span><span class="n">strides</span><span class="p">)</span> <span class="o">-</span> <span class="m">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="m">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span> <span class="p">{</span>
		<span class="n">strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">inter_stride</span>
		<span class="n">inter_stride</span> <span class="o">*=</span> <span class="n">sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">strides</span>
<span class="p">}</span>
</code></pre></div></div>
<p>Lets take a tensor <code class="language-plaintext highlighter-rouge">a</code> with with sizes <code class="language-plaintext highlighter-rouge">[]int{3,5,4}</code>.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">a[0, 0, 0]</code> is in the <code class="language-plaintext highlighter-rouge">data[0]</code> position in underlying slice.</li>
  <li><code class="language-plaintext highlighter-rouge">a[2, 0, 0]</code> is in <code class="language-plaintext highlighter-rouge">data[40] =&gt; 2*4*5=40</code>. For every increase in 1st dimension we go through 20 elements.</li>
</ul>

<h2 id="accessing-the-single-element-with-at-and-set">Accessing the single element with <code class="language-plaintext highlighter-rouge">.At()</code> and <code class="language-plaintext highlighter-rouge">.Set()</code></h2>
<p>To access the single element using the indexes, we multiply and sum all the indexes and strides:</p>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">Tensor</span><span class="p">)</span> <span class="n">At</span><span class="p">(</span><span class="n">idx</span> <span class="o">...</span><span class="kt">int</span><span class="p">)</span> <span class="kt">float64</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Sizes</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">log</span><span class="o">.</span><span class="n">Fatal</span><span class="p">(</span><span class="s">"Too many indices to access"</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="n">ind</span> <span class="o">:=</span> <span class="m">0</span>
	<span class="k">for</span> <span class="n">i</span> <span class="o">:=</span> <span class="m">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Sizes</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span> <span class="p">{</span>
		<span class="n">ind</span> <span class="o">+=</span> <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">Strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Data</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">log</span><span class="o">.</span><span class="n">Fatal</span><span class="p">(</span><span class="s">"The index is too high"</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">Data</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">Offset</span><span class="o">+</span><span class="n">ind</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>
<p>We can create the <code class="language-plaintext highlighter-rouge">At()</code> method similar way by assigning new value to <code class="language-plaintext highlighter-rouge">t.Data[ind]</code>.</p>

<p>So the tensor with sizes <code class="language-plaintext highlighter-rouge">[3,4,5]</code>, <code class="language-plaintext highlighter-rouge">a.At(1,2,4)</code> translates to <code class="language-plaintext highlighter-rouge">data[3*1+4*2+5*4] = data[21]</code>.
Here is the test for this function:</p>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="n">TestMatrixAt</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">testing</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">t</span><span class="o">.</span><span class="n">Run</span><span class="p">(</span><span class="s">"many dimensional"</span><span class="p">,</span> <span class="k">func</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">testing</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">a</span> <span class="o">:=</span> <span class="n">NewTensor</span><span class="p">([]</span><span class="kt">float64</span><span class="p">{</span><span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span> <span class="m">8</span><span class="p">},</span> <span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">})</span>
		<span class="n">got</span> <span class="o">:=</span> <span class="n">a</span><span class="o">.</span><span class="n">At</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
		<span class="n">want</span> <span class="o">:=</span> <span class="m">6.0</span>
		<span class="k">if</span> <span class="n">got</span> <span class="o">!=</span> <span class="n">want</span> <span class="p">{</span>
			<span class="n">t</span><span class="o">.</span><span class="n">Errorf</span><span class="p">(</span><span class="s">"wanted %f, got %f"</span><span class="p">,</span> <span class="n">want</span><span class="p">,</span> <span class="n">got</span><span class="p">)</span>
		<span class="p">}</span>
	<span class="p">})</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="viewing-the-tensor-with-different-sizes">Viewing the tensor with different sizes</h2>
<p>Changing the size of the tensor only involves recalculating the strides without changes the underlying data at all. The only requirement is that the number of elements of the new and old sizes should be equal.</p>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">Tensor</span><span class="p">)</span> <span class="n">View</span><span class="p">(</span><span class="n">idx</span> <span class="o">...</span><span class="kt">int</span><span class="p">)</span> <span class="o">*</span><span class="n">Tensor</span> <span class="p">{</span>
  <span class="c">//prod(...idx) calculate the multiplies of elements idx[0]*idx[1]...idx[n]</span>
	<span class="k">if</span> <span class="n">prod</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Data</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">log</span><span class="o">.</span><span class="n">Fatal</span><span class="p">(</span><span class="s">"The dimensions do not match number of elements"</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="n">strides</span> <span class="o">:=</span> <span class="n">calcualteStrides</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">{</span>
		<span class="n">Data</span><span class="o">:</span>    <span class="n">t</span><span class="o">.</span><span class="n">Data</span><span class="p">,</span>
		<span class="n">Sizes</span><span class="o">:</span>   <span class="n">idx</span><span class="p">,</span>
		<span class="n">Strides</span><span class="o">:</span> <span class="n">strides</span><span class="p">,</span>
		<span class="n">Offset</span><span class="o">:</span>  <span class="m">0</span><span class="p">,</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<p>We can test whether the returned tensor and original tensor share the same storage:</p>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="n">TestMatrixView</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">testing</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">t</span><span class="o">.</span><span class="n">Run</span><span class="p">(</span><span class="s">"test view storage"</span><span class="p">,</span> <span class="k">func</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">testing</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">a</span> <span class="o">:=</span> <span class="n">NewTensor</span><span class="p">([]</span><span class="kt">float64</span><span class="p">{</span><span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span> <span class="m">8</span><span class="p">},</span> <span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">})</span>
		<span class="n">b</span> <span class="o">:=</span> <span class="n">a</span><span class="o">.</span><span class="n">View</span><span class="p">(</span><span class="m">4</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span>
		<span class="n">a</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="m">12</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
		<span class="n">got</span> <span class="o">:=</span> <span class="n">b</span><span class="o">.</span><span class="n">At</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
		<span class="n">want</span> <span class="o">:=</span> <span class="m">12.0</span>
		<span class="k">if</span> <span class="n">got</span> <span class="o">!=</span> <span class="n">want</span> <span class="p">{</span>
			<span class="n">t</span><span class="o">.</span><span class="n">Errorf</span><span class="p">(</span><span class="s">"wanted %f, got %f"</span><span class="p">,</span> <span class="n">want</span><span class="p">,</span> <span class="n">got</span><span class="p">)</span>
		<span class="p">}</span>
	<span class="p">})</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="slicing-the-tensor-along-certain-dimension">Slicing the tensor along certain dimension</h2>
<p>By far the most interesting implementation was slicing the tensor. In numpy-like api we can use <code class="language-plaintext highlighter-rouge">arr[:, 3, :]</code> to get the slice of the 2nd row of 3rd dimension. In this case we have to update all three <code class="language-plaintext highlighter-rouge">Strides, Sizes, Offset</code> attributes since the both size and number of elements change in the resulting tensor. 
<strong>We should use the same underlying array without copying any elements</strong>.
We could implement it in two steps.</p>
<ol>
  <li>Create new <code class="language-plaintext highlighter-rouge">Sizes</code> ignoring the slicing dimension</li>
  <li>Calculate <code class="language-plaintext highlighter-rouge">Strides</code> and <code class="language-plaintext highlighter-rouge">Offset</code> for the resulting tensor
When calculating the <code class="language-plaintext highlighter-rouge">Strides</code> we should ignore the slicing dimension but the order of the strides should be the same. 
For example: tensor <code class="language-plaintext highlighter-rouge">a</code> with sizes <code class="language-plaintext highlighter-rouge">[2,5,6]</code>, if we slice <code class="language-plaintext highlighter-rouge">a[:, 4, :]</code> we will get the new tensor with <code class="language-plaintext highlighter-rouge">size: [2,6], stride: [60, 30]</code>.</li>
</ol>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// Returns the view along the dimension with ind</span>
<span class="k">func</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">Tensor</span><span class="p">)</span> <span class="n">DimSlice</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">ind</span> <span class="kt">int</span><span class="p">)</span> <span class="o">*</span><span class="n">Tensor</span> <span class="p">{</span>
	<span class="n">newSizes</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Sizes</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">i</span> <span class="o">:=</span> <span class="k">range</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Sizes</span><span class="p">)</span> <span class="o">-</span> <span class="m">1</span> <span class="p">{</span>
		<span class="n">newSizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="p">}</span>
	<span class="n">newStrides</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Strides</span><span class="p">)</span><span class="o">-</span><span class="m">1</span><span class="p">)</span>
	<span class="n">ii</span> <span class="o">:=</span> <span class="m">0</span>
	<span class="k">for</span> <span class="n">i</span> <span class="o">:=</span> <span class="m">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">Strides</span><span class="p">);</span> <span class="n">i</span><span class="o">++</span> <span class="p">{</span>
		<span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">dim</span> <span class="p">{</span>
			<span class="n">newStrides</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
			<span class="n">ii</span><span class="o">++</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="n">newOffset</span> <span class="o">:=</span> <span class="n">t</span><span class="o">.</span><span class="n">Strides</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">ind</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">{</span>
		<span class="n">Data</span><span class="o">:</span>    <span class="n">t</span><span class="o">.</span><span class="n">Data</span><span class="p">,</span>
		<span class="n">Sizes</span><span class="o">:</span>   <span class="n">newSizes</span><span class="p">,</span>
		<span class="n">Strides</span><span class="o">:</span> <span class="n">newStrides</span><span class="p">,</span>
		<span class="n">Offset</span><span class="o">:</span>  <span class="n">newOffset</span><span class="p">,</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Here is the testing code for the <code class="language-plaintext highlighter-rouge">DimSlice</code> function:</p>
<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">func</span> <span class="n">TestDimSlice</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">testing</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">a</span> <span class="o">:=</span> <span class="n">NewTensor</span><span class="p">([]</span><span class="kt">float64</span><span class="p">{</span><span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span> <span class="m">8</span><span class="p">},</span> <span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">})</span>
  <span class="c">/*
			tensor([[[1, 2],
					[3, 4]],
          [[5,6],
          [7,8]]])
		*/</span>
	<span class="n">t</span><span class="o">.</span><span class="n">Run</span><span class="p">(</span><span class="s">"test getting 2 dim"</span><span class="p">,</span> <span class="k">func</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span><span class="n">testing</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">b</span> <span class="o">:=</span> <span class="n">a</span><span class="o">.</span><span class="n">DimSlice</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
		<span class="c">/*
			tensor([[2, 4],
					[6, 8]])
		*/</span>
		<span class="n">got</span> <span class="o">:=</span> <span class="n">b</span><span class="o">.</span><span class="n">At</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">)</span>
		<span class="n">want</span> <span class="o">:=</span> <span class="m">6.0</span>
		<span class="k">if</span> <span class="n">got</span> <span class="o">!=</span> <span class="n">want</span> <span class="p">{</span>
			<span class="n">t</span><span class="o">.</span><span class="n">Errorf</span><span class="p">(</span><span class="s">"wanted %f, got %f"</span><span class="p">,</span> <span class="n">want</span><span class="p">,</span> <span class="n">got</span><span class="p">)</span>
		<span class="p">}</span>
	<span class="p">})</span>
</code></pre></div></div>

<p>I wholehardely recommend reading the original blogpost, it has much more detailed and in-depth information that is far beyond the scope of this blogpost. Thanks for reading this far :)</p>]]></content><author><name></name></author><category term="pytorch" /><category term="go" /><category term="implementation" /><summary type="html"><![CDATA[Generic Tensor implementation and testable code for At, View, DimSlice with the same underlying storage (like Pytorch).]]></summary></entry><entry><title type="html">How to make Obsidian and Jekyll equations compatible</title><link href="https://tursunboev.com/blog/2024/obsidian-jekyll-mathjax-single-dollar/" rel="alternate" type="text/html" title="How to make Obsidian and Jekyll equations compatible" /><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2024/obsidian-jekyll-mathjax-single-dollar</id><content type="html" xml:base="https://tursunboev.com/blog/2024/obsidian-jekyll-mathjax-single-dollar/"><![CDATA[<h1 id="tldr">TL;DR</h1>
<p>Set <code class="language-plaintext highlighter-rouge">processEscapes</code> to <code class="language-plaintext highlighter-rouge">False</code> or remove it from <code class="language-plaintext highlighter-rouge">tax</code> dictionary</p>
<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;script&gt;</span>
<span class="nx">MathJax</span> <span class="o">=</span> <span class="p">{</span>
  <span class="na">tex</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">inlineMath</span><span class="p">:</span> <span class="p">[[</span><span class="dl">'</span><span class="s1">$</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">$</span><span class="dl">'</span><span class="p">],</span> <span class="p">[</span><span class="dl">'</span><span class="se">\\</span><span class="s1">(</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="se">\\</span><span class="s1">)</span><span class="dl">'</span><span class="p">]],</span>
  <span class="p">},</span>
  <span class="na">svg</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">fontCache</span><span class="p">:</span> <span class="dl">'</span><span class="s1">global</span><span class="dl">'</span>
  <span class="p">}</span>
<span class="p">};</span>
<span class="nt">&lt;/script&gt;</span>
<span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"text/javascript"</span> <span class="na">id=</span><span class="s">"MathJax-script"</span> <span class="na">async</span>
  <span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"</span><span class="nt">&gt;</span>
<span class="nt">&lt;/script&gt;</span>
</code></pre></div></div>

<h1 id="what-was-not-working">What was not working</h1>
<p>I’ve tried to write blog posts using Obsidian for my Jekyll website. The issue was the Mathjax version I was using didn’t recognize single dollar sign for inline equations.</p>

<p>I was using this Mathjax setup taken from the documentation:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;script&gt;</span>
<span class="nx">MathJax</span> <span class="o">=</span> <span class="p">{</span>
  <span class="na">tex</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">inlineMath</span><span class="p">:</span> <span class="p">[[</span><span class="dl">'</span><span class="s1">$</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">$</span><span class="dl">'</span><span class="p">],</span> <span class="p">[</span><span class="dl">'</span><span class="se">\\</span><span class="s1">(</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="se">\\</span><span class="s1">)</span><span class="dl">'</span><span class="p">]],</span>
    <span class="na">processEscapes</span><span class="p">:</span> <span class="kc">true</span>
  <span class="p">},</span>
  <span class="na">svg</span><span class="p">:</span> <span class="p">{</span>
    <span class="na">fontCache</span><span class="p">:</span> <span class="dl">'</span><span class="s1">global</span><span class="dl">'</span>
  <span class="p">}</span>
<span class="p">};</span>
<span class="nt">&lt;/script&gt;</span>
<span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"text/javascript"</span> <span class="na">id=</span><span class="s">"MathJax-script"</span> <span class="na">async</span>
  <span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"</span><span class="nt">&gt;</span>
<span class="nt">&lt;/script&gt;</span>
</code></pre></div></div>
<p>But the \$ signs in my markdown was displaying as \$ in the rendered HTML. Some of the related answers mentioned the issue of markdown + Mathjax processing order.</p>

<p>I’ve tried different <code class="language-plaintext highlighter-rouge">inlineMath</code> options, but at the end downgrading to <code class="language-plaintext highlighter-rouge">2.x</code> version solved the problem.</p>
<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"text/javascript"</span> <span class="na">async</span> <span class="na">src=</span><span class="s">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"</span><span class="nt">&gt;</span>
 <span class="nt">&lt;/script&gt;</span>
 <span class="nt">&lt;script </span><span class="na">type=</span><span class="s">"text/x-mathjax-config"</span><span class="nt">&gt;</span>
    <span class="nx">MathJax</span><span class="p">.</span><span class="nx">Hub</span><span class="p">.</span><span class="nc">Config</span><span class="p">({</span>
      <span class="na">extensions</span><span class="p">:</span> <span class="p">[</span><span class="dl">"</span><span class="s2">tex2jax.js</span><span class="dl">"</span><span class="p">],</span>
      <span class="na">jax</span><span class="p">:</span> <span class="p">[</span><span class="dl">"</span><span class="s2">input/TeX</span><span class="dl">"</span><span class="p">,</span> <span class="dl">"</span><span class="s2">output/HTML-CSS</span><span class="dl">"</span><span class="p">],</span>
      <span class="na">tex2jax</span><span class="p">:</span> <span class="p">{</span>
        <span class="na">inlineMath</span><span class="p">:</span> <span class="p">[</span> <span class="p">[</span><span class="dl">'</span><span class="s1">$</span><span class="dl">'</span><span class="p">,</span><span class="dl">'</span><span class="s1">$</span><span class="dl">'</span><span class="p">],</span> <span class="p">[</span><span class="dl">"</span><span class="se">\\</span><span class="s2">(</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="se">\\</span><span class="s2">)</span><span class="dl">"</span><span class="p">]</span> <span class="p">],</span>
        <span class="na">displayMath</span><span class="p">:</span> <span class="p">[</span> <span class="p">[</span><span class="dl">'</span><span class="s1">$$</span><span class="dl">'</span><span class="p">,</span><span class="dl">'</span><span class="s1">$$</span><span class="dl">'</span><span class="p">],</span> <span class="p">[</span><span class="dl">"</span><span class="se">\\</span><span class="s2">[</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="se">\\</span><span class="s2">]</span><span class="dl">"</span><span class="p">]</span> <span class="p">],</span>
      <span class="p">},</span>
      <span class="dl">"</span><span class="s2">HTML-CSS</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span> <span class="na">availableFonts</span><span class="p">:</span> <span class="p">[</span><span class="dl">"</span><span class="s2">TeX</span><span class="dl">"</span><span class="p">]</span> <span class="p">}</span>
    <span class="p">});</span>
 <span class="nt">&lt;/script&gt;</span>
</code></pre></div></div>

<p>However after trying with options later, turns out just removing <code class="language-plaintext highlighter-rouge">processEscapes</code> seem to solve the problem.</p>]]></content><author><name></name></author><category term="obsidian" /><category term="jekyll" /><summary type="html"><![CDATA[Fixing the single dollar display compatibility between Obsidian and Jekyll]]></summary></entry><entry><title type="html">ActMAD : Activation Matching to Align Distributions for Test-Time-Training paper review</title><link href="https://tursunboev.com/blog/2023/actmad-activation-matching-tta/" rel="alternate" type="text/html" title="ActMAD : Activation Matching to Align Distributions for Test-Time-Training paper review" /><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/actmad-activation-matching-tta</id><content type="html" xml:base="https://tursunboev.com/blog/2023/actmad-activation-matching-tta/"><![CDATA[<h1 id="test-time-adaptation">Test-time adaptation</h1>

<p>Test-time adaptation is one of the emerging topics in tackling distribution shift in model deployment. Typically, the lifecycle of the model deployment includes followings:</p>
<ol>
  <li>(Pre)-Training the model on the training dataset offline.</li>
  <li>Deploying the model in real world</li>
  <li>After getting some more data, further retrain the model</li>
  <li>Repeat 2,3 steps</li>
</ol>

<p>Some of the issues with the above steps are:</p>
<ul>
  <li>The environment might change during the deployment period, so the model might lose its performance over time. For example, model trained on the images of streets in clear weather will not work as well in fog or snow.</li>
  <li>Retraining the model might require access to the original dataset, which we may have access to. E.g. ViT model trained on JFT-300M</li>
  <li>Even if we only “adapt” the model using the acquired data, we still need to label them for training.</li>
</ul>

<p>Test time adaptation (TTA) techniques adapt the source model to the distribution of test data, during the testing phase. It will not require access to the source training data and only updates the model using the unlabeled test data.
Also the test data is given in a stream, so we won’t even have access to the entire test dataset. Only the current batch of examples are used for adaptation. This actually matches the real-world deployment case, where the models lifetime includes different test distributions over time.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/20230706000002-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/20230706000002-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/20230706000002-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/20230706000002.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://arxiv.org/abs/2203.13591</div>

<p>Most TTA techniques fall into one of the three categories:</p>
<ul>
  <li><strong>Normalization based methods</strong>: (Only updating the parameters or statistics of the BatchNorm layer)</li>
  <li><strong>Entropy minimization</strong>: fine-tunes the model by minimizing the prediction entropy of the model</li>
  <li><strong>Prototype-based methods</strong>: Modifies the linear classifier so that it maps the input to the embedding space and trains it using the protype representations of each class for prediction.</li>
</ul>

<h1 id="actmad-activation-matching-to-align-distributions-for-test-time-training">ActMAD: Activation Matching to Align Distributions for Test-Time-Training</h1>

<p>The problem with most of the existing methods is they have constraints on the type of the model and task. For example, normalization based methods mostly work on models with BatchNorm layers. Other methods use some kind of classification based loss for adapting the model.</p>

<h2 id="proposed-solution">Proposed solution</h2>
<p><a href="https://arxiv.org/abs/2211.12870"><strong>ActMAD</strong> paper</a> proposes a versatile TTA method which works on any model or task regardless of their design structure. It works by aligning the distribution of individual features across the network. Previous feature alignment techniques used the distribution feature maps as a group. <strong>ActMAD</strong> takes each individual feature vector and aligns it with the source model. Since the features have position awareness, it allows the network to adapt in fine-grained detail depending on the location of the object in interest. For example, roads are usually on the bottom of the image so the features on the bottom have different distribution than the feature on the top.</p>

<h2 id="feature-alignmentactivation-matching">Feature alignment/activation matching</h2>
<p>During the test phase we keep the original source model $\theta^*$ and adapt our model $\theta$ by taking the statistics of the activation outputs after the normalization layer.</p>

<p>Let say the activation layer $l$ ‘s output is $a_l = norm(conv(a_{l-1}))$. Its mean and variances can be calculated across the test batch. We pre-compute the source models activation statistics on the training dataset and have $\hat{\mu_l}$ and $\hat{\sigma_l}$ for each layer. In test time we compute the loss at layer $l$ as:</p>

\[\begin{equation} L_l(\theta) = |\mu_l - \hat{\mu}_l| + |\sigma^2_l - \hat{\sigma}^2_l| \end{equation}\]

<p>Overall loss is simply the sum of all losses across the layers.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/20230706002758-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/20230706002758-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/20230706002758-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/20230706002758.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://arxiv.org/abs/2211.12870</div>

<h2 id="experimental-results">Experimental results</h2>

<p>The performance gain of <strong>ActMAD</strong> is the most significant in <a href="https://www.cvlibs.net/datasets/kitti/">KITTI dataset</a>  (~10 percent). It is likely because KITTI is constructed from the road images which is more structured that CIFAR or ImageNet. Also the proposed method can be combined with other entropy-based methods.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/20230706003759-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/20230706003759-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/20230706003759-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/20230706003759.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://arxiv.org/abs/2211.12870</div>

<p>One of the main advantages is it works in object detection and image segmentation tasks with any model, which most of the TTA methods do not consider. Usual datasets for comparison in TTA being CIFAR, ImageNet corruption or other classification tasks.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The proposed method takes different approach in TTA by being fully compatible with any task or architecture. It is especially great in data with location dependent structures: road images, selfies etc.
One drawback is it requires the statistics of the activations from the source data which may not be available.</p>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-notes" /><category term="tta" /><summary type="html"><![CDATA[Test-time adaptation introduction and review of the ActMAD paper]]></summary></entry><entry><title type="html">LoRA : Low-Rank Adaptation of Large Language Models</title><link href="https://tursunboev.com/blog/2023/low-rank-adaptation-of-large-llms/" rel="alternate" type="text/html" title="LoRA : Low-Rank Adaptation of Large Language Models" /><published>2023-05-30T00:00:00+00:00</published><updated>2023-05-30T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/low-rank-adaptation-of-large-llms</id><content type="html" xml:base="https://tursunboev.com/blog/2023/low-rank-adaptation-of-large-llms/"><![CDATA[<p><strong>Paper link:</strong> <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p>

<p>Instead of updating the pre-trained model weights $W_0$ directly, low-rank decomposition matrices added $W_0 + BA$ where $B,A \in R^{d \times r}$  and only $BA$ is finetuned keeping $W_0$ is frozen. After the training, they can be added to get final model.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lora-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lora-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lora-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lora.png" class="img-fluid rounded z-depth-1 center" width="60%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h1 id="advantages">Advantages</h1>
<ul>
  <li>Large pre-trained model weights are not changed during fine-tuning.</li>
  <li>Inference is the same, $Wx = W_0x + BAx = (W_0+BA)x$</li>
  <li>It is efficient and have small training footprint (Only $BA$ matrices are trained)</li>
  <li>Swapping the models in deployment is fast. $W = (W - BA) + B^\prime A^\prime$</li>
</ul>

<h1 id="detailed-notes">Detailed notes</h1>
<p>When adopting LLMs into downstream tasks, they are fine-tuned to learn those tasks. If we fine-tune entire network, it will have huge training and storage costs and each task will require separate model.</p>

<p>The goal is to use small number of parameters when adopting to new tasks. The hypothesis is that the updates to the network during fine-tuning have low “intrinsic rank”. So we can limit the weight updates to the low-rank decomposition $W_0 + \Delta W = W_0 + BA$, where $B, A \in R^{d\times r}, r«min(d,k)$ 
If the $r=d$ then it represents the full model update so it can recover full training in theory.</p>

<p>For multiple tasks during inference, multiple $(BA)_t$ can be swapped for each task $t$ while keeping the large $W_0$ in memory. For reference $BA$ has $2d$ number of parameters while $W_0$ has $d^2$.</p>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-notes" /><category term="transformers" /><summary type="html"><![CDATA[Paper link: https://arxiv.org/abs/2106.09685]]></summary></entry><entry><title type="html">Few-shot image classification using Prototypical Networks</title><link href="https://tursunboev.com/blog/2023/prototipycal-networs-few-shot/" rel="alternate" type="text/html" title="Few-shot image classification using Prototypical Networks" /><published>2023-05-26T00:00:00+00:00</published><updated>2023-05-26T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/prototipycal-networs-few-shot</id><content type="html" xml:base="https://tursunboev.com/blog/2023/prototipycal-networs-few-shot/"><![CDATA[<p>In few-shot image classification problem, $K$ examples of $N$ types of images are given as an “support” set and the task is to classify the new images by comparing them to those images.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/protoypical-network-training-example-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/protoypical-network-training-example-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/protoypical-network-training-example-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/protoypical-network-training-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://cs330.stanford.edu/lecture_slides/cs330_transfer_meta_learning.pdf</div>

<p>One of the simplest methods is <a href="https://arxiv.org/abs/1703.05175">Prototypical Networks</a>. During training we feed both “support” and “query” images to the CNN encoder model. Then we take the center of each image class in “support set” and compare the distance of “query” (test) images. We assign the label of the closest center point (prototype) to each “query” image.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prototypical-network-clustering-example-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prototypical-network-clustering-example-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prototypical-network-clustering-example-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prototypical-network-clustering-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://arxiv.org/abs/1703.05175</div>

<p>Nice property of this method is it acts the same in training and test time. It also don’t require any additional parameters for meta-training.</p>

<p>Basic inner training procedure in Pytorch can be implemented follows: <a href="https://github.com/gladuz/Basic-Protonet-Implementation-Pytorch/blob/master/Working.ipynb">Jupyter notebook in Github</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#.... training loop
</span>
<span class="c1"># We have N types of images with K examples each for training and 1 for testing
#For the image above we have K=5, N=3
</span><span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1">#BxK+1xNxD
</span><span class="n">support_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1"># BxKxNxD
</span><span class="n">query_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1"># Bx1xNxD 
</span><span class="n">query_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">N</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">prototypes</span> <span class="o">=</span> <span class="n">support_logits</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#BxNxD
</span><span class="n">query_dist</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">cdist</span><span class="p">(</span><span class="n">query_logits</span><span class="p">,</span> <span class="n">prototypes</span><span class="p">)</span> <span class="c1">#Negative distances (closer better)
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">query_dist</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">)</span>

<span class="c1">#.... Backprop, accuracy etc.
</span></code></pre></div></div>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-notes" /><summary type="html"><![CDATA[Basic implementation exampleof prototypical networks in Pytorch]]></summary></entry><entry><title type="html">Hungarian matching algorithm in DETR</title><link href="https://tursunboev.com/blog/2023/hungarian-matching-detr/" rel="alternate" type="text/html" title="Hungarian matching algorithm in DETR" /><published>2023-04-20T00:00:00+00:00</published><updated>2023-04-20T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/hungarian-matching-detr</id><content type="html" xml:base="https://tursunboev.com/blog/2023/hungarian-matching-detr/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>In the <a href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers</a> paper, they directly predict $N$ number of prediction boxes and treat them as set. To find the matching predicted boxes with the target boxes they use <em>Hungarian matching</em> algorithm. There is a <a href="https://leimao.github.io/blog/Hungarian-Matching-Algorithm/">great blogpost by Lei Mao</a> explaining the basic concepts of Hungarian matching algorithm.</p>

<h1 id="short-summary-of-the-problem">Short summary of the problem</h1>
<p>In the case of DETR, we predict 100 boxes which is more than maximum number of boxes almost any image. Our task is to find closest* predicted box for each target box. Meaning, we will select $n$ best prediction boxes among the $m$ outputs.</p>

<p>To do this, we form a cost matrix $C$ with the size $m \times n$ , where $m$ is the number of predictions and $n$ is the number of targets where $m &gt; n$ . $C_{i,j}$ would be the matching cost of prediction $i$ and ground truth box $j$ .</p>

<h1 id="matching-cost">Matching cost</h1>
<p>Matching cost of the element $C_{ij}$ is given by:</p>

\[\begin{equation}
C_{ij} = \mathcal{L}_{iou}(b_i, \hat{b}_j) + ||b_i - \hat{b}_j||_1 - \hat{p}_j(c_i)
\end{equation}\]

<p>where $\hat{p}_j(c_i)$ is the probability of the target class. 
After calculating the cost matrix $C$, we can use <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html#scipy.optimize.linear_sum_assignment">linar sum assignment</a> function from the SciPy package. It returns the <em>row_ids</em> and <em>column_ids</em> which corresponds to the (matched_predictions_ids, target_ids).</p>

<h1 id="code">Code</h1>
<p>Following is the simplified version of the Hungarian matching algorithm used in the <a href="https://github.com/facebookresearch/detr/blob/main/models/matcher.py">source code of DETR</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">linear_sum_assignment</span>

<span class="n">target_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># two target labels
</span><span class="n">target_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># two target boxes
</span>
<span class="n">pred_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 10 predictions for 3 labels
</span><span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="err"> </span><span class="c1"># 4 boxes for 10 predictions
</span>
<span class="n">class_cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">pred_logits</span><span class="p">[:,</span> <span class="n">target_labels</span><span class="p">]</span> <span class="c1"># 10x2
# We can use torch.cdist which returns the norm distance matrix 10x2
</span><span class="n">l1_cost</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cdist</span><span class="p">(</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">target_bboxes</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 10x2
# To simplify we omit the IoU calculation. Look at https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/models/matcher.py#L74
</span><span class="n">iou_cost</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 10x2
</span>
<span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">class_cost</span> <span class="o">+</span> <span class="n">l1_cost</span> <span class="o">+</span> <span class="n">iou_cost</span>
<span class="n">match_preds</span><span class="p">,</span> <span class="n">match_targets</span> <span class="o">=</span> <span class="nf">linear_sum_assignment</span><span class="p">(</span><span class="n">cost_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">match_preds</span><span class="p">,</span> <span class="n">match_targets</span><span class="p">)</span>
<span class="c1">#[2 9] [0 1]
</span></code></pre></div></div>
<p>In this example, predictions $2,9$ matched with $0,1$ target boxes, respecitvely : (2&lt;-&gt;0), (9&lt;-&gt;1)</p>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-notes" /><summary type="html"><![CDATA[Explaining Hungarian matching algorithm used in DETR with a small example]]></summary></entry></feed>
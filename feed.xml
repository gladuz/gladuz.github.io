<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tursunboev.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tursunboev.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-30T16:36:33+00:00</updated><id>https://tursunboev.com/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Jamshid Tursunboev
</subtitle><entry><title type="html">LoRA : Low-Rank Adaptation of Large Language Models</title><link href="https://tursunboev.com/blog/2023/low-rank-adaptation-of-large-llms/" rel="alternate" type="text/html" title="LoRA : Low-Rank Adaptation of Large Language Models" /><published>2023-05-30T00:00:00+00:00</published><updated>2023-05-30T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/low-rank-adaptation-of-large-llms</id><content type="html" xml:base="https://tursunboev.com/blog/2023/low-rank-adaptation-of-large-llms/"><![CDATA[<p><strong>Paper link:</strong> <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p>

<p>Instead of updating the pre-trained model weights \(W_0\) directly, low-rank decomposition matrices added \(W_0 + BA\) where \(B,A \in R^{d \times r}\)  and only \(BA\) is finetuned keeping \(W_0\) is frozen. After the training, they can be added to get final model.</p>
<h1 id="advantages">Advantages</h1>
<ul>
  <li>Large pre-trained model weights are not changed during fine-tuning.</li>
  <li>Inference is the same, \(Wx = W_0x + BAx = (W_0+BA)x\)</li>
  <li>It is efficient and have small training footprint (Only \(BA\) matrices are trained)</li>
  <li>Swapping the models in deployment is fast. \(W = (W - BA) + B^\prime A^\prime\)</li>
</ul>

<h1 id="detailed-notes">Detailed notes</h1>
<p>When adopting LLMs into downstream tasks, they are fine-tuned to learn those tasks. If we fine-tune entire network, it will have huge training and storage costs and each task will require separate model.</p>

<p>The goal is to use small number of parameters when adopting to new tasks. The hypothesis is that the updates to the network during fine-tuning have low “intrinsic rank”. So we can limit the weight updates to the low-rank decomposition \(W_0 + \Delta W = W_0 + BA\), where \(B, A \in R^{d\times r}, r&lt;&lt;min(d,k)\) 
If the \(r=d\) then it represents the full model update so it can recover full training in theory.</p>

<p>For multiple tasks during inference, multiple \((BA)_t\) can be swapped for each task \(t\) while keeping the large \(W_0\) in memory. For reference \(BA\) has \(2d\) number of parameters while \(W_0\) has \(d^2\).</p>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-note" /><category term="transformers" /><summary type="html"><![CDATA[Paper link: https://arxiv.org/abs/2106.09685]]></summary></entry><entry><title type="html">Few-shot image classification using Prototypical Networks</title><link href="https://tursunboev.com/blog/2023/prototipycal-networs-few-shot/" rel="alternate" type="text/html" title="Few-shot image classification using Prototypical Networks" /><published>2023-05-26T00:00:00+00:00</published><updated>2023-05-26T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/prototipycal-networs-few-shot</id><content type="html" xml:base="https://tursunboev.com/blog/2023/prototipycal-networs-few-shot/"><![CDATA[<p>In few-shot image classification problem, \(K\) examples of \(N\) types of images are given as an “support” set and the task is to classify the new images by comparing them to those images.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/protoypical-network-training-example-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/protoypical-network-training-example-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/protoypical-network-training-example-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/protoypical-network-training-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://cs330.stanford.edu/lecture_slides/cs330_transfer_meta_learning.pdf</div>

<p>One of the simplest methods is <a href="https://arxiv.org/abs/1703.05175">Prototypical Networks</a>. During training we feed both “support” and “query” images to the CNN encoder model. Then we take the center of each image class in “support set” and compare the distance of “query” (test) images. We assign the label of the closest center point (prototype) to each “query” image.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prototypical-network-clustering-example-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prototypical-network-clustering-example-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prototypical-network-clustering-example-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prototypical-network-clustering-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">https://arxiv.org/abs/1703.05175</div>

<p>Nice property of this method is it acts the same in training and test time. It also don’t require any additional parameters for meta-training.</p>

<p>Basic inner training procedure in Pytorch can be implemented follows: <a href="https://github.com/gladuz/Basic-Protonet-Implementation-Pytorch/blob/master/Working.ipynb">Jupyter notebook in Github</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#.... training loop
</span>
<span class="c1"># We have N types of images with K examples each for training and 1 for testing
#For the image above we have K=5, N=3
</span><span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1">#BxK+1xNxD
</span><span class="n">support_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1"># BxKxNxD
</span><span class="n">query_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1"># Bx1xNxD 
</span><span class="n">query_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">N</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">prototypes</span> <span class="o">=</span> <span class="n">support_logits</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#BxNxD
</span><span class="n">query_dist</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">cdist</span><span class="p">(</span><span class="n">query_logits</span><span class="p">,</span> <span class="n">prototypes</span><span class="p">)</span> <span class="c1">#Negative distances (closer better)
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">query_dist</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">)</span>

<span class="c1">#.... Backprop, accuracy etc.
</span></code></pre></div></div>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-note" /><summary type="html"><![CDATA[Basic implementation exampleof prototypical networks in Pytorch]]></summary></entry><entry><title type="html">Hungarian matching algorithm in DETR</title><link href="https://tursunboev.com/blog/2023/hungarian-matching-detr/" rel="alternate" type="text/html" title="Hungarian matching algorithm in DETR" /><published>2023-04-20T00:00:00+00:00</published><updated>2023-04-20T00:00:00+00:00</updated><id>https://tursunboev.com/blog/2023/hungarian-matching-detr</id><content type="html" xml:base="https://tursunboev.com/blog/2023/hungarian-matching-detr/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>In the <a href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers</a> paper, they directly predict \(N\) number of prediction boxes and treat them as set. To find the matching predicted boxes with the target boxes they use <em>Hungarian matching</em> algorithm. There is a <a href="https://leimao.github.io/blog/Hungarian-Matching-Algorithm/">great blogpost by Lei Mao</a> explaining the basic concepts of Hungarian matching algorithm.</p>

<h1 id="short-summary-of-the-problem">Short summary of the problem</h1>
<p>In the case of DETR, we predict 100 boxes which is more than maximum number of boxes almost any image. Our task is to find closest* predicted box for each target box. Meaning, we will select \(n\) best prediction boxes among the \(m\) outputs.</p>

<p>To do this, we form a cost matrix \(C\) with the size \(m \times n\) , where \(m\) is the number of predictions and \(n\) is the number of targets where \(m &gt; n\) . \(C_{i,j}\) would be the matching cost of prediction \(i\) and ground truth box \(j\) .</p>

<h1 id="matching-cost">Matching cost</h1>
<p>Matching cost of the element \(C_{ij}\) is given by:</p>

\[\begin{equation}
C_{ij} = \mathcal{L}_{iou}(b_i, \hat{b}_j) + ||b_i - \hat{b}_j||_1 - \hat{p}_j(c_i)
\end{equation}\]

<p>where \(\hat{p}_j(c_i)\) is the probability of the target class. 
After calculating the cost matrix \(C\), we can use <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html#scipy.optimize.linear_sum_assignment">linar sum assignment</a> function from the SciPy package. It returns the <em>row_ids</em> and <em>column_ids</em> which corresponds to the (matched_predictions_ids, target_ids).</p>

<h1 id="code">Code</h1>
<p>Following is the simplified version of the Hungarian matching algorithm used in the <a href="https://github.com/facebookresearch/detr/blob/main/models/matcher.py">source code of DETR</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">linear_sum_assignment</span>

<span class="n">target_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># two target labels
</span><span class="n">target_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># two target boxes
</span>
<span class="n">pred_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 10 predictions for 3 labels
</span><span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="err"> </span><span class="c1"># 4 boxes for 10 predictions
</span>
<span class="n">class_cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">pred_logits</span><span class="p">[:,</span> <span class="n">target_labels</span><span class="p">]</span> <span class="c1"># 10x2
# We can use torch.cdist which returns the norm distance matrix 10x2
</span><span class="n">l1_cost</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cdist</span><span class="p">(</span><span class="n">pred_bboxes</span><span class="p">,</span> <span class="n">target_bboxes</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 10x2
# To simplify we omit the IoU calculation. Look at https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/models/matcher.py#L74
</span><span class="n">iou_cost</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 10x2
</span>
<span class="n">cost_matrix</span> <span class="o">=</span> <span class="n">class_cost</span> <span class="o">+</span> <span class="n">l1_cost</span> <span class="o">+</span> <span class="n">iou_cost</span>
<span class="n">match_preds</span><span class="p">,</span> <span class="n">match_targets</span> <span class="o">=</span> <span class="nf">linear_sum_assignment</span><span class="p">(</span><span class="n">cost_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">match_preds</span><span class="p">,</span> <span class="n">match_targets</span><span class="p">)</span>
<span class="c1">#[2 9] [0 1]
</span></code></pre></div></div>
<p>In this example, predictions \(2,9\) matched with \(0,1\) target boxes, respecitvely : (2&lt;-&gt;0), (9&lt;-&gt;1)</p>]]></content><author><name></name></author><category term="deep-learning" /><category term="paper-note" /><summary type="html"><![CDATA[Explaining Hungarian matching algorithm used in DETR with a small example]]></summary></entry></feed>